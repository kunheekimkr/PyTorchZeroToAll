{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: CNN\n",
    " \n",
    "## Structure\n",
    "CNN (Convolutional Neural Network)\n",
    "\n",
    "Input -> Feature Extraction (convolution + subsampling) -> Classification (fully connected, Dense Net) -> Output\n",
    "\n",
    "## Convolution\n",
    "\n",
    "Cut a small \"Patch\" from the whole data and do operations to get one value.\n",
    "\n",
    "Example) cut 3x3x1 input data with 2x2x1 filter, stride 1x1 :=> 2x2x1 data is generated \n",
    "\n",
    "\n",
    "$$\n",
    "Example =  \\begin{bmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "4 & 5 & 6\\\\\n",
    "7 & 8 & 9 \n",
    "\\end{bmatrix} \n",
    ",\n",
    "Filter = \\begin{bmatrix}\n",
    "0.1 & 0.5 \\\\\n",
    "0.3 & 0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Result = \\begin{bmatrix}\n",
    "4.3 & 5.6 \\\\\n",
    "8.2 & 9.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$ ex) 1 * 0.1  + 2 * 0.5 + 4 * 0.3 + 5 * 0.4 = 4.3 , 2 * 0.1  + 3 * 0.5 + 5 * 0.3 + 6 * 0.4 = 5.6 , ... $$\n",
    "\n",
    "Usually, dot product $$ w^T x +b $$ is used.\n",
    "\n",
    "\n",
    "## ConvNet\n",
    "\n",
    "For Example, Operating with a 32px * 32px image:\n",
    "\n",
    "Each pixel has a RGB Data, so it the input data size is 32 * 32 *3.\n",
    "If we had 6 5 * 5 * 3 filters, we would get 6 seperate 28 * 28 activation maps. (28 * 28 * 6 data)\n",
    "\n",
    "ConvNet is a Sequence of Convolutional Layers, interspersed with activation functions.\n",
    "\n",
    "## Pooling\n",
    "\n",
    "Max Pooling : Choose the maximum data from a certain filter size to compress the data.\n",
    "\n",
    "Example: Max pooling with 2x2 filter and stride 2\n",
    "\n",
    "$$\n",
    "Example =  \\begin{bmatrix}\n",
    "1 & 1 & 2 & 4\\\\\n",
    "5 & 6 & 7 & 8\\\\\n",
    "3 & 2 & 1 & 0\\\\\n",
    "1 & 2 & 3 & 4 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "$$\n",
    "Result = \\begin{bmatrix}\n",
    "6 & 8 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Average Pooling : Calculate the average data from a certain filter size to compress the data.\n",
    "\n",
    "Example: Average pooling with 2x2 filter and stride 2\n",
    "\n",
    "$$\n",
    "Example =  \\begin{bmatrix}\n",
    "1 & 1 & 2 & 4\\\\\n",
    "5 & 6 & 7 & 8\\\\\n",
    "3 & 2 & 1 & 0\\\\\n",
    "1 & 2 & 3 & 4 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "$$\n",
    "Result = \\begin{bmatrix}\n",
    "3.25 & 5.25 \\\\\n",
    "2 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Simple CNN\n",
    "\n",
    "In CNN, Features are Localy Connected!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb Cell 2'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=79'>80</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m===========================\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTest set: Average loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mcorrect\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39mdataset)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=80'>81</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m100.\u001b[39m \u001b[39m*\u001b[39m correct \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39mdataset)\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=82'>83</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=83'>84</a>\u001b[0m     train(epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=84'>85</a>\u001b[0m     test()\n",
      "\u001b[1;32m/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb Cell 2'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=54'>55</a>\u001b[0m data, target \u001b[39m=\u001b[39m Variable(data), Variable(target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=55'>56</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=56'>57</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=57'>58</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=58'>59</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb Cell 2'\u001b[0m in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=41'>42</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(in_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# Flatten the tnesor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=42'>43</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kunheekim/Desktop/workspace/PyTorchZeroToAll/lec_10.ipynb#ch0000004?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlog\u001b[39m.\u001b[39msoftmax(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.functional' has no attribute 'log'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Training Settings\n",
    "batch_size=64\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# Model Design\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super (Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc = nn.Linear(320 , 10) # 320 -> 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1) # Flatten the tnesor\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "        # get the index of the max log-porbability\n",
    "        pred = torch.max(output.data, 1)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
